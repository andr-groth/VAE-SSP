{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore VAE on SSP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook demonstrates the process of exploring a variational auto-encoder (VAE) model trained on Shared Socioeconomic Pathways (SSPs) data from CMIP6. The process involves various steps, including model and data loading, analysis of the latent space, model output generation, and reconstruction in grid space. The key steps and components involved are outlined as follows:\n",
    "\n",
    "1. The configuration parameters of the model are loaded from the `LOG_DIR` folder.\n",
    "\n",
    "2. The VAE model consists of four components: _encoder_, _latent sampling_, _decoder_, and a _second decoder for prediction_. Separate model instances are created for each component:\n",
    "    * _Encoder_: The encoder takes a sample `x` and returns the mean `z_mean` and logarithmic variance `z_log_var` of the latent variable `z`.\n",
    "    * _Latent Sampling_: The latent sampling takes `z_mean` and `z_log_var` as inputs and generates a random latent sample `z`.\n",
    "    * _Decoder_: The decoder reconstructs the input `x` by taking the latent sample `z` and producing the decoded output `y`. The decoding is done backward in time, maintaining the input order.\n",
    "   * _Decoder for Prediction_: The second decoder also takes the latent sample `z` but generates a forward-time prediction output.\n",
    "   \n",
    "3. The model weights from the training process are loaded from the `LOG_DIR` folder.\n",
    "\n",
    "4. CMIP data is loaded from netCDF files, with different variables stacked along the channel axis.\n",
    "\n",
    "5. Forcing data is loaded from a CSV file used for model training. Additional forcing data is loaded from a second CSV file, which will be used as a new trajectory of the condition for the decoder and prediction.\n",
    "\n",
    "6. The `encoder` and `decoder` properties are explored. KL divergence, histograms, and temporal behavior of the latent variables are analyzed. Invariance of the latent space with respect to forcing is tested.\n",
    "\n",
    "7. The model outputs are obtained for the CMIP data and the new forcing data. The outputs of the `decoder` and `prediction` are collected separately and aligned with the target month. The VAE output is restricted to specific time lags for reducing file size.\n",
    "\n",
    "8. The model output is projected into grid space by forming the scalar product with the EOFs (Empirical Orthogonal Functions).The corresponding climatological mean fields are loaded and added to obtain absolute values.\n",
    "\n",
    "9. Selected model runs with the highest KL divergence are used to form the ensemble forecast. Various ensemble statistics are exported as as netCDF files in the specified `EXPORT_DIR` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as ks\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "from matplotlib import dates\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import get_logger\n",
    "from tensorflow.compat.v1 import disable_eager_execution, disable_v2_behavior\n",
    "\n",
    "get_logger().setLevel('ERROR')\n",
    "disable_eager_execution()\n",
    "disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from VAE import generators, models\n",
    "from VAE.utils import fileio\n",
    "from VAE.utils import plot as vplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIGWIDTH = 12\n",
    "VERBOSE = 1\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['figure.dpi'] = 75\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: f'{x: .3f}'}, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the configuration from the the folder `LOG_DIR`. The model output is written to netCDF files in the folder given in `EXPORT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "LOG_DIR = r'logs/2023-05-30T18.58'\n",
    "MODEL_FILE = f'model.{EPOCH:02d}.h5'\n",
    "EXPORT_DIR = r'results/2023-05-30T18.58'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('LOG_DIR    :', LOG_DIR)\n",
    "print('MODEL_FILE :', MODEL_FILE)\n",
    "print('EXPORT_DIR :', EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the parameters from the model training in `trainer_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(LOG_DIR, 'trainer_config.yaml')\n",
    "with open(fn, 'r') as f:\n",
    "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "print('Load configuration from:', os.path.normpath(fn))\n",
    "\n",
    "assert params['model'].get('__version__') == models.__version__, 'Model version mismatch.'\n",
    "assert params['fit_generator'].get('__version__') == generators.__version__, 'Generator version mismatch.'\n",
    "\n",
    "params = SimpleNamespace(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some modifications to the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.model['beta'] = 1.  # no beta scheduler needed at inference time\n",
    "params.fit_generator['shuffle'] = False  # do not shuffle samples\n",
    "# params.fit_generator['repeat_samples'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(yaml.dump(params.__dict__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model consists of four components: encoder, latent sampling, decoder, and a second decoder for prediction. Separate model instances are created for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes a sample `x` and returns `z_mean` and `z_log_var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Encoder(**params.model, name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent sampling takes the two inputs `z_mean` and `z_log_var` and returns a set of `set_size=1` random latent sample `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sampling = models.LatentSampling(**params.model, name='latent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder, finally, takes a latent sample `z` and returns the decoded output `y` to reconstruct `x`. The decoding works backward in time and we set `output_reverse=True` so that the order of decoder output matches the input to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = models.Decoder(output_shape=params.model.get('input_shape'),\n",
    "                         decoder_blocks=params.model.get('encoder_blocks'),\n",
    "                         output_reverse=True,\n",
    "                         **params.model,\n",
    "                         name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the decoder, the second decoder takes the same latent sample `z` and it's output will provide the prediction. In contrast to the `decoder`, we set `output_reverse=False` so that the output of `prediction` is forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = models.Decoder(output_shape=params.model.get('prediction_shape'),\n",
    "                            output_reverse=False,\n",
    "                            **{\n",
    "                                'decoder_blocks': params.model.get('encoder_blocks'),\n",
    "                                **params.model,\n",
    "                                **params.prediction\n",
    "                            },\n",
    "                            name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the four components, we a ready to create the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.VAEp(encoder, decoder, latent_sampling, prediction, **params.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.utils.plot_model(model, show_shapes=True, dpi=75, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model weights from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fn = os.path.join(LOG_DIR, MODEL_FILE)\n",
    "model.load_weights(fn, by_name=True)\n",
    "print('Load model weights from:', os.path.normpath(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMIP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the netCDF files of CMIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_variables, _dimensions, _attributes = fileio.read_netcdf_multi(**params.data, num2date=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the netCDF files and their variables by the global attributes `source_id` + `experiment_id`. The attribute `source_id` refers to the model name (e.g. `ACCESS-CM2`) and the attribute `experiment_id` to the experiment (e.g. `ssp126`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {}\n",
    "dimensions = {}\n",
    "attributes = {}\n",
    "\n",
    "key1 = 'source_id'\n",
    "key2 = 'experiment_id'\n",
    "for dataset_name, values in _variables.items():\n",
    "    target_key = (\n",
    "        _attributes[dataset_name]['.'][key1],\n",
    "        _attributes[dataset_name]['.'][key2],\n",
    "    )\n",
    "\n",
    "    variables.setdefault(target_key, {})\n",
    "    dimensions.setdefault(target_key, {})\n",
    "    attributes.setdefault(target_key, {})\n",
    "\n",
    "    variables[target_key] |= {k: pd.DataFrame(v, index=_dimensions[dataset_name]['time']) for k, v in values.items()}\n",
    "    dimensions[target_key] |= {k: _dimensions[dataset_name] for k in values.keys()}\n",
    "    attributes[target_key] |= {k: _attributes[dataset_name] for k in values.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a few tests to check the integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = {tuple(val.keys()) for val in variables.values()}\n",
    "if len(variable_names) > 1:\n",
    "    raise ValueError(f'More than one variable combination found: {variable_names}')\n",
    "else:\n",
    "    variable_names, *_ = variable_names\n",
    "    print('\\N{check mark} One variable combination found:', variable_names)\n",
    "\n",
    "variable_channels = {tuple(v.shape[-1] for v in val.values()) for val in variables.values()}\n",
    "if len(variable_channels) > 1:\n",
    "    raise ValueError(f'More than one channel combination found: {variable_channels}')\n",
    "else:\n",
    "    variable_channels, *_ = variable_channels\n",
    "    print('\\N{check mark} One channel combination found:', variable_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the models and their different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of model runs found :', len(variables))\n",
    "df = pd.crosstab(*list(zip(*list(variables.keys()))), rownames=[key1], colnames=[key2])\n",
    "df.loc['--- Total ---'] = df.sum(axis=0)\n",
    "display(df.replace(0, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we compare the different CMIP datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 3\n",
    "rows = 2\n",
    "\n",
    "dataset_names = list(variables.keys())\n",
    "_, first_idx, color_idx = np.unique([dataset_name[1] for dataset_name in dataset_names],\n",
    "                                    return_index=True,\n",
    "                                    return_inverse=True)\n",
    "\n",
    "cm = plt.cm.get_cmap('tab10', 10)\n",
    "color_dict = {name: cm(idx) for name, idx in zip(dataset_names, color_idx)}\n",
    "\n",
    "for variable_name in variable_names:\n",
    "    fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(FIGWIDTH, 3 * rows), squeeze=False)\n",
    "    fig.suptitle(variable_name.upper(), fontweight='bold')\n",
    "    for idx, dataset_name in enumerate(dataset_names):\n",
    "        for channel, (ax, values) in enumerate(zip(axs.flat, variables[dataset_name][variable_name].values.T)):\n",
    "            label = dataset_name[1] if idx in first_idx else None\n",
    "            ax.plot(variables[dataset_name][variable_name].index,\n",
    "                    values,\n",
    "                    color=color_dict[dataset_name],\n",
    "                    label=label,\n",
    "                    alpha=0.5)\n",
    "\n",
    "            ax.set_title(f'Channel {channel}')\n",
    "            ax.grid(visible=True, linestyle=':')\n",
    "            locator = dates.YearLocator(25)\n",
    "            ax.xaxis.set_major_formatter(dates.ConciseDateFormatter(locator))\n",
    "            ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    axs.flat[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables are stack along the last axis, the channels. We add a leading singleton dimension to `dataset` for `set_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names_rand = list(variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_stack = [\n",
    "    pd.concat([variables[dataset_name][variable_name] for variable_name in variable_names], axis=1, join='inner')\n",
    "    for dataset_name in dataset_names_rand\n",
    "]\n",
    "\n",
    "time = [d.index for d in data_stack]\n",
    "dataset = [d.to_numpy()[None, ...] for d in data_stack]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training, the dataset was furthermore split into one set for model training and one set for model validation. At inference time, we keep the entier dataset in order to generate results for all CMIP runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0\n",
    "print('Size of training dataset   :', len(dataset[:validation_split]))\n",
    "print('Size of validation dataset :', len(dataset[validation_split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSP forcings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the forcing data from a csv file used to train the model. The first column is considered as the date and the remaining columns as the corresponding forcing data. The column header of the forcing data must match the `experiment_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = params.forcing['filename']\n",
    "forcing_df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "print('Load forcing data from:', os.path.relpath(filename))\n",
    "display(forcing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forcing data is interpolated so that it can be resampled at the time points of the CMIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_df_daily = forcing_df.asfreq('D').interpolate(method='akima')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of providing only the current value of the forcing as condition, we can optionally provide past values as well. The parameter `input_length` in `params.forcing` determines the number of past values (in years) that will be provided as condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = params.forcing.get('scale', 1)\n",
    "input_length = params.forcing.get('input_length', 1)\n",
    "\n",
    "forcing_data = [\n",
    "    np.stack([\n",
    "        forcing_df_daily[experiment_id].reindex(t - pd.DateOffset(years=years), method='nearest').to_numpy() * scale\n",
    "        for years in range(input_length)\n",
    "    ],\n",
    "             axis=1) for t, (_, experiment_id) in zip(time, dataset_names_rand)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New forcings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We furthermore load forcing data from a second csv file that will be used as condition for `decoder` and `prediction`. The first column is considered as the date and the second column as the corresponding forcing data. The new forcing data is the same for all dataset, i.e. the same for different `experiment_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = r'data\\cmip6\\ssp\\forcings\\SSP_jumps_world_C02.csv'\n",
    "\n",
    "forcing2_df = pd.read_csv(filename2, index_col=0, parse_dates=True)\n",
    "print('Load forcing data from:', os.path.relpath(filename2))\n",
    "display(forcing2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new forcing data is likewise interpolated and embedded as the original forcings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing2_df_daily = forcing2_df.asfreq('D').interpolate(method='akima')\n",
    "forcing2_name, *_ = forcing2_df_daily.keys()\n",
    "\n",
    "forcing2_data = [\n",
    "    np.stack([\n",
    "        forcing2_df_daily[forcing2_name].reindex(t - pd.DateOffset(years=years), method='nearest').to_numpy() * scale\n",
    "        for years in range(input_length)\n",
    "    ],\n",
    "             axis=1) for t in time\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare forcings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the original SSP forcings with the modified forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(FIGWIDTH / 2, FIGWIDTH / 3))\n",
    "for n, (forcing_name, val) in enumerate(forcing_df.items()):\n",
    "    ax.plot(val, 'o', color=cm(n), label=forcing_name)\n",
    "    ax.plot(forcing_df_daily[forcing_name], '-', color=cm(n))\n",
    "\n",
    "new_color = cm(len(forcing_df) + 1)\n",
    "for n, (forcing_name, val) in enumerate(forcing2_df.items()):\n",
    "    ax.plot(val, 'o', color=new_color, label=forcing_name)\n",
    "    ax.plot(forcing2_df_daily[forcing_name], '-', color=new_color, linewidth=3)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation generator takes the full set of CMIP data.\n",
    "\n",
    "During training, the same CO2 concentration data was provided as condition to the encoder and decoder corresponding to the input and target data they were trained on. During inference, here, different values of CO2 concentration data are provided as input to the decoder. This allows the VAE to generate forecasts for different CO2 concentration scenarios while sampling from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [t.to_numpy().astype('datetime64[M]').astype(int) for t in time]\n",
    "_, model_index = np.unique([dataset_name[0] for dataset_name in dataset_names_rand], return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generators.FitGenerator(dataset[validation_split:],\n",
    "                                  **params.fit_generator,\n",
    "                                  time=month[validation_split:],\n",
    "                                  ensemble_index=model_index[validation_split:],\n",
    "                                  condition={\n",
    "                                      'encoder': forcing_data[validation_split:],\n",
    "                                      'decoder': forcing2_data[validation_split:]\n",
    "                                  })\n",
    "val_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The module `VAE.utils.plot` provides multiple functions to plot and analyze properties of the `encoder` and the `decoder`. First let's start with the `encoder` and explore properties of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0, figsize=(FIGWIDTH, 6))\n",
    "fig, ax, z_order, kl_div = vplt.encoder_boxplot(encoder, val_gen, plottype='kl', name=0, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the KL divergence of the latent variables for each of the latent dimension separately. The dimensions are sorted in descending order of the KL divergence. Latent dimensions with a high KL divergence are more important for the reconstruction with the decoder. Latent dimensions that have a KL divergence close to zero are unused dimensions; i.e. they are practically not important for the reconstruction.\n",
    "\n",
    "A good property is when we observe a separation into a few important and vanishing (zero KL) dimensions, which means that the VAE is operating in a so-called __polarized regime__. This avoids overfitting with too many latent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_units = np.sum(np.mean(kl_div, axis=0) > 0.1)\n",
    "print('Number of active units:', active_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Temporal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyze the temporal behavior of the latent variables. In doing so, we obtain the latent variables of the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var = encoder.predict(val_gen, verbose=VERBOSE)\n",
    "z_sample = latent_sampling.predict([z_mean, z_log_var])\n",
    "z_sample = np.squeeze(z_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent variables are split into the different model runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen_splits = np.cumsum([(len(t) - val_gen.input_length - val_gen.prediction_length + 1) * val_gen.repeat_samples\n",
    "                            for t in time[validation_split:]][:-1])\n",
    "\n",
    "z_sample_list = np.split(z_sample, val_gen_splits, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = 7\n",
    "fig, axs = plt.subplots(rows,\n",
    "                        2,\n",
    "                        figsize=(FIGWIDTH, 2 * rows),\n",
    "                        sharex='col',\n",
    "                        sharey='col',\n",
    "                        squeeze=False,\n",
    "                        gridspec_kw={\n",
    "                            'width_ratios': [2.5, 1],\n",
    "                            'wspace': 0.1\n",
    "                        })\n",
    "\n",
    "r = val_gen.repeat_samples\n",
    "nfft = 2**12\n",
    "fs = 12\n",
    "\n",
    "for idx, (t, z,\n",
    "          dataset_name) in enumerate(zip(time[validation_split:], z_sample_list,\n",
    "                                         dataset_names_rand[validation_split:])):\n",
    "    label = dataset_name[1] if idx in first_idx else None\n",
    "    for (lax, rax), k in zip(axs, z_order):\n",
    "        lax.plot(t[val_gen.input_length:-val_gen.prediction_length + 1],\n",
    "                 z[:, k].reshape(-1, r).mean(axis=-1),\n",
    "                 color=color_dict[dataset_name])\n",
    "        lax.set_ylabel(f'{k=}')\n",
    "        lax.grid(axis='x', linestyle=':')\n",
    "\n",
    "        f, pxx = signal.welch(z[:, k].reshape(-1, r).mean(axis=-1),\n",
    "                              nfft=nfft,\n",
    "                              fs=fs,\n",
    "                              nperseg=512,\n",
    "                              axis=0,\n",
    "                              scaling='spectrum')\n",
    "        rax.plot(f, pxx, color=color_dict[dataset_name], label=label)\n",
    "\n",
    "axs[0, 1].legend()\n",
    "lax.margins(x=0.005)\n",
    "locator = dates.YearLocator(10)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "lax.xaxis.set_major_formatter(dates.ConciseDateFormatter(locator))\n",
    "lax.xaxis.set_minor_locator(dates.YearLocator(2))\n",
    "\n",
    "rax.set_xlim((0, 0.7))\n",
    "rax.xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "rax.xaxis.set_minor_locator(ticker.AutoMinorLocator(4))\n",
    "_ = rax.set_xlabel('Cycles per year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the temporal dynamics of the latent variables and the corresponding power spectrum. The success of disentanglement can be seen in a clear spearation between different dynamics, e.g. different power spectra. In the leading three latent dimensions, we can identify a low-frequency component and an oscillatory pair. The oscillatory pair is characterized by similar power specra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_trend = getattr(params, '__notes__', {}).get('k_trend', z_order[0])\n",
    "k_pair = getattr(params, '__notes__', {}).get('k_pair', z_order[[1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test invariance of latent space wrt forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we test whether the leading latent dimensions are not specific to the different forcings. If this is the case, the VAE has learned a posterior that is invariant to the different forcings. This, in turn, is important if we want the output of the VAE to depend only on the forcings, while drawing a random sample from the posterior aggregrated over different forcings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forcing vs. low-frequency component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we analyze the low-frequency component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name = variable_names[1]\n",
    "k_pc = 0\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(FIGWIDTH, FIGWIDTH * 0.75))\n",
    "kw_args = dict(marker='.', markersize=2, linestyle='none')\n",
    "for idx, (dataset_name, forcing,\n",
    "          z) in enumerate(zip(dataset_names_rand[validation_split:], forcing_data[validation_split:], z_sample_list)):\n",
    "    forcing = np.atleast_2d(forcing)[:, 0] / params.forcing.get('scale', 1)\n",
    "    data = variables[dataset_name][variable_name].values\n",
    "    label = dataset_name[1] if idx in first_idx else None\n",
    "    axs[0, 0].plot(data[val_gen.input_length:-val_gen.prediction_length + 1, k_pc],\n",
    "                   forcing[val_gen.input_length:-val_gen.prediction_length + 1],\n",
    "                   color=color_dict[dataset_name],\n",
    "                   label=label,\n",
    "                   **kw_args)\n",
    "\n",
    "    axs[0, 1].plot(z[:, k_trend].reshape(-1, r),\n",
    "                   forcing[val_gen.input_length:-val_gen.prediction_length + 1],\n",
    "                   color=color_dict[dataset_name],\n",
    "                   **kw_args)\n",
    "\n",
    "    axs[1, 1].plot(z[:, k_trend].reshape(-1, r),\n",
    "                   data[val_gen.input_length:-val_gen.prediction_length + 1, k_pc],\n",
    "                   color=color_dict[dataset_name],\n",
    "                   **kw_args)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.grid(linestyle=':')\n",
    "axs[1, 0].remove()\n",
    "\n",
    "axs[0, 0].legend(markerscale=10, loc='lower right')\n",
    "axs[0, 0].set_ylabel('Forcing')\n",
    "axs[1, 1].set_xlabel(f'z(k={k_trend})')\n",
    "axs[0, 0].set_xlabel(f'{variable_name.upper()}   channel {k_pc + 1}')\n",
    "axs[1, 1].set_ylabel(f'{variable_name.upper()}   channel {k_pc + 1}')\n",
    "\n",
    "invert = np.mean([z[-1, k_trend] for z in z_sample_list]) < 0\n",
    "if invert:\n",
    "    axs[0, 1].invert_xaxis()\n",
    "    axs[1, 1].invert_xaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upper left panel, we compare the forcing with the leading PC in the SST, i.e. the trend in the SST. There is clearly a strong relationship between the two. In the upper right panel, we compare the forcing with the low frequency component. We see that the distributions of the $z$ values have a strong overlap, indicating success of the VAE in learning an invariant representation of the low-frequency dynamics. In the lower right panel, the low-frequency component is compared to the SST trend. We see that the SST trend does not depend on the identified low-frequency component, indicating that the VAE has identified low-frequency dynamics that is not specific to the forcing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forcing vs. oscillatory mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyze the oscillatory pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k0, k1 = k_pair\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(FIGWIDTH * 0.5, FIGWIDTH * 0.5))\n",
    "for dataset_name, forcing, z in zip(dataset_names_rand[validation_split:], forcing_data[validation_split:],\n",
    "                                    z_sample_list):\n",
    "    forcing = np.atleast_2d(forcing)[:, 0] / params.forcing.get('scale', 1)\n",
    "    ax.plot(z[:, k0].reshape(-1, r).mean(axis=-1),\n",
    "            z[:, k1].reshape(-1, r).mean(axis=-1),\n",
    "            linestyle='-',\n",
    "            color=color_dict[dataset_name])\n",
    "\n",
    "ax.set_xlabel(f'z(k={k0})')\n",
    "ax.set_ylabel(f'z(k={k1})')\n",
    "ax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distributions of the $z$ values have a strong overlap, indicating success of the VAE in learning an invariant representation of the oscillatory dynamics as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first obtain the model outputs from the input data given to `val_gen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcs, ycs = model.predict(val_gen, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decoder` and `prediction` outputs, `xcs` and `ycs`, are concatenated along the lag/lead dimension and the singleton dimension for `set_size=1` is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcs = np.concatenate([xcs, ycs], axis=2)\n",
    "xcs = np.squeeze(xcs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output is split into the different model runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcs_list = np.split(xcs, val_gen_splits, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the model output is aligned with the target month and split into the different variables. To reduce the later size of the netCDF files, we restrict the VAE output to specific time lags given in `export_lags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_lags = [-1]\n",
    "# export_lags = np.arange(-val_gen.input_length, val_gen.prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_splits = np.cumsum(variable_channels)\n",
    "level = np.arange(-val_gen.input_length, val_gen.prediction_length)\n",
    "lag_idx = val_gen.input_length + np.array(export_lags)\n",
    "\n",
    "level = level[lag_idx]\n",
    "\n",
    "r = val_gen.repeat_samples\n",
    "xcs_variables = {}\n",
    "xcs_dimensions = {}\n",
    "xcs_attributes = {}\n",
    "for dataset_name, values, t in zip(dataset_names_rand[validation_split:], xcs_list, time[validation_split:]):\n",
    "    # restrict to given time lags\n",
    "    values = values[:, lag_idx, :]\n",
    "    # average over repeat samples\n",
    "    values = values.reshape(-1, r, *values.shape[1:]).mean(axis=1)\n",
    "    # align  with target month\n",
    "    values = np.pad(values, ((val_gen.input_length, val_gen.prediction_length - 1), (0, 0), (0, 0)),\n",
    "                    mode='constant',\n",
    "                    constant_values=np.nan)\n",
    "    values = np.stack([np.roll(values[:, n, :], lag, axis=0) for n, lag in enumerate(level)], axis=1)\n",
    "    # split channels into variables\n",
    "    splits = np.split(values, channel_splits, axis=-1)\n",
    "\n",
    "    xcs_variables[dataset_name] = dict(zip(variable_names, splits))\n",
    "    xcs_dimensions[dataset_name] = {variable_name: {'time': t, 'level': level} for variable_name in variable_names}\n",
    "    xcs_attributes[dataset_name] = {\n",
    "        variable_name: {\n",
    "            'level': {\n",
    "                'long_name': 'Time lag',\n",
    "                'units': '',\n",
    "                'axis': 'Z'\n",
    "            }\n",
    "        }\n",
    "        for variable_name in variable_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we show examples of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_idx = np.searchsorted(export_lags, -1)\n",
    "cols = 3\n",
    "rows = 2\n",
    "\n",
    "for variable_name in variable_names:\n",
    "    fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(FIGWIDTH, 3 * rows), squeeze=False)\n",
    "    fig.suptitle(variable_name.upper(), fontweight='bold')\n",
    "    for idx, dataset_name in enumerate(dataset_names_rand[validation_split:]):\n",
    "        for channel, (ax, values) in enumerate(zip(axs.flat, xcs_variables[dataset_name][variable_name].T)):\n",
    "            label = dataset_name[1] if idx in first_idx else None\n",
    "            ax.plot(xcs_dimensions[dataset_name][variable_name]['time'],\n",
    "                    values.T[:, lag_idx],\n",
    "                    color=color_dict[dataset_name],\n",
    "                    label=label,\n",
    "                    alpha=0.5)\n",
    "\n",
    "            ax.set_title(f'Channel {channel}')\n",
    "            ax.grid(visible=True, linestyle=':')\n",
    "            locator = dates.YearLocator(25)\n",
    "            ax.xaxis.set_major_formatter(dates.ConciseDateFormatter(locator))\n",
    "            ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "    axs.flat[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the decoder is provided with a different CO2 concentration scenario than the one used for the encoder, the decoder output differs from the input data to the encoder. Instead, the decoder output provides a forecast for the given CO2 concentration scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction in grid space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the model output is projected into the grid space by forming the scalar product of the model output with the EOFs. The reults are exported as netCDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the EOFs from the `eofs.nc` files, which can also be found in the data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eof_files = [os.path.join(os.path.dirname(filename), 'eofs.nc') for filename in params.data['filename']]\n",
    "_eof_variables, _eof_dimensions, _eof_attributes = fileio.read_netcdf_multi(filename=eof_files,\n",
    "                                                                            time_range=params.data.get('level_range'),\n",
    "                                                                            dtype=params.data.get('dtype'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eof_variables = {}\n",
    "eof_dimensions = {}\n",
    "eof_attributes = {}\n",
    "for dataset_name, values in _eof_variables.items():\n",
    "    eof_variables |= values\n",
    "\n",
    "    eof_dimensions |= {k: _eof_dimensions[dataset_name] for k in values}\n",
    "    eof_attributes |= {k: _eof_attributes[dataset_name] for k in values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load climatological mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain absolute values, we also load the corresponding climatological mean fields. The netCDF files will be looked up in the folder `mean_path`, relative to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_path = '../mean/*.nc'\n",
    "mean_files = [os.path.join(os.path.dirname(filename), mean_path) for filename in params.data['filename']]\n",
    "_mean_variables, _mean_dimensions, _mean_attributes = fileio.read_netcdf_multi(filename=mean_files,\n",
    "                                                                               num2date=True,\n",
    "                                                                               dtype=params.data.get('dtype'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the netCDF files and their variables by the global attributes `source_id` + `experiment_id`, as for the CMIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_variables = {}\n",
    "mean_dimensions = {}\n",
    "mean_attributes = {}\n",
    "\n",
    "key1 = 'source_id'\n",
    "key2 = 'experiment_id'\n",
    "for dataset_name, values in _mean_variables.items():\n",
    "    target_key = (\n",
    "        _mean_attributes[dataset_name]['.'][key1],\n",
    "        _mean_attributes[dataset_name]['.'][key2],\n",
    "    )\n",
    "\n",
    "    mean_variables.setdefault(target_key, {})\n",
    "    mean_dimensions.setdefault(target_key, {})\n",
    "    mean_attributes.setdefault(target_key, {})\n",
    "\n",
    "    mean_variables[target_key] |= values\n",
    "    mean_dimensions[target_key] |= {k: _mean_dimensions[dataset_name] for k in values}\n",
    "    mean_attributes[target_key] |= {k: _mean_attributes[dataset_name] for k in values}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict the export of netCDF files to model runs with highest KL divergence, i.e. the model runs that are most important in the construction of the aggregated posterior. We exclude the low-frequency component from the calculation of the mean KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_div_list = np.split(kl_div, val_gen_splits, axis=0)\n",
    "k = list(set(z_order) - set([k_trend]))\n",
    "# k = z_order\n",
    "kl_div_mean = np.array([kl[:, k].mean() for kl in kl_div_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids, experiment_ids = list(zip(*dataset_names_rand))\n",
    "\n",
    "df = pd.DataFrame({'source_id': source_ids, 'experiment_id': experiment_ids, 'KL div': kl_div_mean})\n",
    "df = df.pivot(values='KL div', index='source_id', columns='experiment_id')\n",
    "mean = df.mean(axis=1).sort_values(ascending=False)\n",
    "df = df.reindex(mean.index)\n",
    "\n",
    "display(\n",
    "    df.style.background_gradient(\n",
    "        'coolwarm',\n",
    "        text_color_threshold=0,\n",
    "        axis=0,\n",
    "    ).highlight_quantile(\n",
    "        axis=0,\n",
    "        q_left=(len(df) - topk) / len(df),\n",
    "        props='font-weight:900',\n",
    "    ).format(precision=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows the KL divergence averaged over the different model runs. In each scenario, we select the `topk` model runs with the highest KL divergence (in bold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names_topk = []\n",
    "for column in df:\n",
    "    names = df[column].dropna().sort_values(ascending=False).iloc[:topk].index.tolist()\n",
    "    dataset_names_topk += [(name, column) for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to netCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We form the dot product of the selected model outputs with the EOFs and add the climatological mean. Optionally, the log transform is reverted. The result is then written to netCDF files in the folder initially given in `EXPORT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(EXPORT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sup_time = np.unique(np.concatenate(time).astype('datetime64[M]'))\n",
    "\n",
    "for variable_name in variable_names:\n",
    "    print('-' * 3, variable_name, '-' * (77 - len(variable_name)))\n",
    "    filename = '{prefix:s}' + variable_name + '.{type:s}.nc'\n",
    "    filename = os.path.join(EXPORT_DIR, filename)\n",
    "\n",
    "    # align model outputs\n",
    "    values = []\n",
    "    for dataset_name in dataset_names_topk:\n",
    "        _value = xcs_variables[dataset_name][variable_name]\n",
    "        _time = xcs_dimensions[dataset_name][variable_name]['time'].to_numpy().astype('datetime64[M]')\n",
    "        idx = np.isin(sup_time, _time, assume_unique=True)\n",
    "        value = np.full_like(_value, np.nan, shape=(len(sup_time), *_value.shape[1:]))\n",
    "        value[idx, ...] = _value\n",
    "\n",
    "        values.append(value)\n",
    "        nc_dimensions = mean_dimensions[dataset_name][variable_name] | dimensions[dataset_name][\n",
    "            variable_name] | xcs_dimensions[dataset_name][variable_name]\n",
    "        nc_attributes = mean_attributes[dataset_name][variable_name] | attributes[dataset_name][\n",
    "            variable_name] | xcs_attributes[dataset_name][variable_name]\n",
    "\n",
    "    values = np.stack(values)\n",
    "\n",
    "    # FIXME: undo scaling in read_netcdf()\n",
    "    # scale = nc_attributes[variable_name].pop('scale', 1.)\n",
    "    # print(scale)\n",
    "    # values /= scale\n",
    "\n",
    "    # scalar product\n",
    "    nc_variables = np.tensordot(values, eof_variables[variable_name], axes=1)\n",
    "\n",
    "    nc_dimensions['time'] = sup_time\n",
    "    kwargs = dict(dimensions=nc_dimensions, attributes=nc_attributes)\n",
    "\n",
    "    # save anomalies\n",
    "    fileio.write_netcdf(filename.format(prefix='anom_', type='ensmean'),\n",
    "                        variables={variable_name: np.mean(nc_variables, axis=0)},\n",
    "                        **kwargs)\n",
    "\n",
    "    prcs = {'ensmedian': 50, 'enspctl10': 10, 'enspctl90': 90}\n",
    "    nc_prcs = np.percentile(nc_variables, list(prcs.values()), axis=0)\n",
    "    for type, value in zip(prcs, nc_prcs):\n",
    "        fileio.write_netcdf(filename.format(prefix='anom_', type=type), variables={variable_name: value}, **kwargs)\n",
    "\n",
    "    # revert to absolute values\n",
    "    months = pd.to_datetime(nc_dimensions['time']).month\n",
    "    for nc_variable, dataset_name in zip(nc_variables, dataset_names_topk):\n",
    "        mean_months = pd.to_datetime(mean_dimensions[dataset_name][variable_name]['time']).month\n",
    "        for month in mean_months:\n",
    "            nc_variable[months == month, ...] += mean_variables[dataset_name][variable_name][mean_months == month, ...]\n",
    "\n",
    "    # invert log transform\n",
    "    if '-log ' in nc_attributes['.']['history']:\n",
    "        nc_variables = np.exp(nc_variables)\n",
    "\n",
    "    # save absolute values\n",
    "    fileio.write_netcdf(filename.format(prefix='', type='ensmean'),\n",
    "                        variables={variable_name: np.mean(nc_variables, axis=0)},\n",
    "                        **kwargs)\n",
    "\n",
    "    nc_prcs = np.percentile(nc_variables, list(prcs.values()), axis=0)\n",
    "    for type, value in zip(prcs, nc_prcs):\n",
    "        fileio.write_netcdf(filename.format(prefix='', type=type), variables={variable_name: value}, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we derive predictions of cumulative rainfall indices covering the growing seasons of the main crops. We first load the data on the growing season from the `data/crop/` folder. The data is assumed to be on the same grid as the EOFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_file = 'data/crop/soy_rf_ggcmi_crop_calendar_phase3_v1.01_1dgr.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_variables, _, _ = fileio.read_netcdf(filename=crop_file)\n",
    "print('Available data:', {k: v.shape for k, v in crop_variables.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the netCDF files with the VAE output for further post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(EXPORT_DIR, variable_names[0] + '*.nc')\n",
    "result_variables, result_dimensions, result_attributes = fileio.read_netcdf_multi(filename=filename, num2date=True)\n",
    "print(*list(result_variables.keys()), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planting_day = crop_variables['planting_day']\n",
    "growing_season_length = crop_variables['growing_season_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the cumulative values for each of the loaded VAE outputs and write the results to netCDF files with the prefix `cum_` preprended to the filename. THe files are saved in the same folder as the VAE output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, values in result_variables.items():\n",
    "    # monthly values refer to end month (M = month end frequency)\n",
    "    source_time = pd.to_datetime(result_dimensions[filename]['time']).snap('M')\n",
    "    # new time is Jan-01 of each year (YS = year start frequency)\n",
    "    target_time = source_time.snap('YS').unique()\n",
    "\n",
    "    # iterate over variables\n",
    "    out_values = dict()\n",
    "    for key, value in values.items():\n",
    "        out_value = np.full((len(target_time), *value.shape[1:]), fill_value=np.nan, dtype=value.dtype)\n",
    "\n",
    "        # iterate over grid points\n",
    "        pbar = ks.utils.Progbar(value.shape[-1], unit_name='Longitude')\n",
    "        for lon_idx in range(value.shape[-1]):\n",
    "            pbar.add(1)\n",
    "            for lat_idx in range(value.shape[-2]):\n",
    "                if np.all(np.isnan(value[..., lat_idx, lon_idx])):\n",
    "                    continue\n",
    "\n",
    "                # converting data to Dataframe makes datetime manipulations easier\n",
    "                df = pd.DataFrame(value[..., lat_idx, lon_idx], index=source_time)\n",
    "\n",
    "                # get total cumulative rainfall at grid point\n",
    "                df = df.cumsum(axis=0)\n",
    "\n",
    "                # interpolate on daily time scales to account for fraction of month (slower computation!)\n",
    "                df = df.asfreq('D').interpolate('linear')\n",
    "\n",
    "                # get start and end dates of crop seasons\n",
    "                start_time = target_time + pd.to_timedelta(planting_day[lat_idx, lon_idx] - 1, unit='D')\n",
    "                end_time = start_time + pd.to_timedelta(growing_season_length[lat_idx, lon_idx], unit='D')\n",
    "\n",
    "                # get cumulative rainfall from difference between end and start of crop season\n",
    "                start_value = df.reindex(index=start_time, method='nearest').to_numpy()\n",
    "                end_value = df.reindex(index=end_time, method='nearest').to_numpy()\n",
    "                out_value[..., lat_idx, lon_idx] = end_value - start_value\n",
    "\n",
    "        out_values[key] = out_value\n",
    "\n",
    "    out_filename = os.path.join(os.path.dirname(filename), 'cum_' + os.path.basename(filename))\n",
    "    fileio.write_netcdf(out_filename,\n",
    "                        variables=out_values,\n",
    "                        dimensions=result_dimensions[filename] | {'time': target_time},\n",
    "                        attributes=result_attributes[filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark on country averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract country averages from the gridded cumulative rainfall data, we can use the CDO operators `fldmean` and `maskregion`. For example, we can use the following command\n",
    "\n",
    "```shell\n",
    "cdo fldmean -maskregion,dcw:TZ infile.nc outfile.nc\n",
    "```\n",
    "to extract the country average for Tanzania. To extract the country average for another country, we can replace `TZ` with the corresponding country code. For more details, see the [CDO documentation](https://code.mpimet.mpg.de/projects/cdo/embedded/cdo.pdf) and the list of [country codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2).\n",
    "\n",
    "To use country codes together with the `maskregion` operator, we need to install the `gmt-dcw` package, which is available in the Ubuntu repositories. To install the package, run `sudo apt install gmt-dcw` and set the environment variable `DCW_DIR` to the path of the `dcw-gmt` folder, e.g. `export DIR_DCW=/usr/share/gmt-dcw/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
