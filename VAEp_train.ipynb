{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE on SSP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook demonstrates the process of training a Variational Autoencoder (VAE) on precipitation and sea-surface temperature data. The model  is trained on different Shared Socio-economic Pathways (SSPs) data of CMIP6. The key steps and components involved are outlined as follows:\n",
    "\n",
    "1. The VAE model consists of four components: encoder, latent sampling, decoder, and a second decoder for prediction. Separate model instances are created for each component:\n",
    "    * _Encoder_ : The encoder takes a sample `x` and returns the mean `z_mean` and logarithmic variance `z_log_var` of the latent variable `z`.\n",
    "    * _Latent Sampling_ : The latent sampling takes `z_mean` and `z_log_var` as inputs and generates a random latent sample `z`.\n",
    "    * _Decoder_ : The decoder reconstructs the input `x` by taking the latent sample `z` and producing the decoded output `y`. The decoding is done backward in time, maintaining the input order.\n",
    "   * _Decoder for Prediction_ : The second decoder also takes the latent sample `z` but generates a forward-time prediction output.\n",
    "\n",
    "2. The full model is created by combining the four components. Model weights and training metrics are saved in a log directory (`LOG_DIR`). If the initial epoch (`INITIAL_EPOCH`) is greater than zero, the training continues from the saved weights; otherwise, a new training session starts.\n",
    "\n",
    "3. The data used in training includes different SSPs of CMIP data as well as the corresponding forcing data. CMIP data is loaded from netCDF files and grouped by model name and run. The forcing data is loaded forcing data from a CSV file. The dataset is split into training and validation sets.\n",
    "\n",
    "4. During training, generators are prepared for training and validation on CMIP data. A beta scheduler is used to scale the KL loss during training. Callbacks are set up to save model weights and metrics.  These callbacks enable tracking the progress of the training and provide checkpoints for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The notebook requires the VAE package to be installed, which is available at:\n",
    "\n",
    "    https://github.com/andr-groth/VAE-project\n",
    "\n",
    "2. Sample data used in the notebook is included in the `data/` folder. The data is in netCDF format and has been prepared with the help of the CDO scripts, which are available at:\n",
    "\n",
    "    https://andr-groth.github.io/cdo-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as ks\n",
    "import tensorflow.keras.backend as K\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from pytz import timezone\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import get_logger\n",
    "from tensorflow.compat.v1 import disable_eager_execution, disable_v2_behavior\n",
    "\n",
    "get_logger().setLevel('ERROR')\n",
    "disable_eager_execution()\n",
    "disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from VAE import generators, models\n",
    "from VAE.callbacks import ModelCheckpoint\n",
    "from VAE.utils import beta_schedulers, fileio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIGWIDTH = 16\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['figure.dpi'] = 75\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: f'{x: .3f}'}, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folder `LOG_DIR`, the model weights and training metrics are saved. The metrics can be monitored with Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_EPOCH = 0\n",
    "LOG_DIR = r'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `INITIAL_EPOCH` to a value greater than zero will continue the training in `LOG_DIR`. Otherwise, a new training is started in a new subfolder created in `LOG_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL_EPOCH = 25\n",
    "# LOG_DIR = os.path.join(LOG_DIR, '2023-05-30T18.58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INITIAL_EPOCH == 0:\n",
    "    tz = timezone('Europe/Berlin')\n",
    "    log_dir = os.path.join(LOG_DIR, datetime.now(tz).strftime('%Y-%m-%dT%H.%M'))\n",
    "    os.makedirs(log_dir)\n",
    "    print('Start training in:', os.path.normpath(log_dir))\n",
    "else:\n",
    "    log_dir = LOG_DIR\n",
    "    assert os.path.exists(log_dir), 'Path not found'\n",
    "    print(f'Continue training from epoch {INITIAL_EPOCH} in:', os.path.normpath(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the parameters of the model and the training (`model`, `data`, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INITIAL_EPOCH == 0:\n",
    "    epochs = 25  # epochs for training\n",
    "    level_range = [20, 20]  # number of PCs that will be used\n",
    "    input_shape = [12, sum(level_range)]\n",
    "    prediction_shape = [12, sum(level_range)]\n",
    "    ensemble_size = 29  # number of CMIP models\n",
    "    repeat_samples = 5  # batch ensemble size\n",
    "    forcing_input_length = 16  # number of past forcing values (in years)\n",
    "    dtype = 'float32'\n",
    "\n",
    "    params = {\n",
    "        # params for encoder and decoder\n",
    "        'model': {\n",
    "            'activation': 'swish',\n",
    "            'beta': 'beta',\n",
    "            'cond_size': [12 + forcing_input_length, ensemble_size],\n",
    "            'cond_units': [64, 24],\n",
    "            'cond_activation': ['swish', 'linear'],\n",
    "            'cond_ens_size': 6,\n",
    "            'cond_use_scale': False,\n",
    "            'delta': 1,\n",
    "            'encoder_blocks': 2,\n",
    "            'fc_units': 64,\n",
    "            'fc_activation': 'tanh',\n",
    "            'film_temporal': True,\n",
    "            'filters': 64,\n",
    "            'gamma': 5,\n",
    "            'input_shape': input_shape,\n",
    "            'latent_dim': 32,\n",
    "            'learning_rate': 2e-3,\n",
    "            'pooling': None,\n",
    "            'prediction_shape': prediction_shape,\n",
    "            'padding_blocks': 1,\n",
    "            'repeat_samples': repeat_samples,\n",
    "            'residual_units': 1,\n",
    "            'set_size': 1,\n",
    "            '__version__': models.__version__,\n",
    "        },\n",
    "        # specific params for prediction model\n",
    "        'prediction': {},\n",
    "        'data': {\n",
    "            'filename': [\n",
    "                'data/cmip6/ssp/pr/pcs/pcs*.nc',\n",
    "                'data/cmip6/ssp/tos/pcs/pcs*.nc',\n",
    "            ],\n",
    "            'level_range': [[level] for level in level_range],\n",
    "            'time_interval': ['2015-01-01', '2100-12-31'],\n",
    "            'dtype': dtype\n",
    "        },\n",
    "        'forcing': {\n",
    "            'filename': 'data/cmip6/ssp/forcings/SSP_CMIP6_world_C02.csv',\n",
    "            'scale': 1e-5,\n",
    "            'input_length': forcing_input_length,\n",
    "        },\n",
    "        'beta_scheduler': {\n",
    "            'class_name': 'LogisticGrowth',\n",
    "            'config': {\n",
    "                'upper': 50,\n",
    "                'midpoint': 5,\n",
    "                'rate': 1,\n",
    "                'dtype': dtype\n",
    "            }\n",
    "        },\n",
    "        'fit_generator': {\n",
    "            'input_length': input_shape[0],\n",
    "            'prediction_length': prediction_shape[0],\n",
    "            'batch_size': 128,\n",
    "            'ensemble_size': ensemble_size,\n",
    "            'ensemble_type': 'index',\n",
    "            'repeat_samples': repeat_samples,\n",
    "            'shuffle': True,\n",
    "            'strides': 1,\n",
    "            'tp_period': 12,\n",
    "            'validation_split': 70,\n",
    "            'random_seed': 2,\n",
    "            'dtype': dtype,\n",
    "            '__version__': generators.__version__,\n",
    "        },\n",
    "        'model_fit': {\n",
    "            'epochs': epochs\n",
    "        },\n",
    "    }\n",
    "\n",
    "    fn = os.path.join(log_dir, 'trainer_config.yaml')\n",
    "    with open(fn, 'w') as f:\n",
    "        yaml.dump(params, f)\n",
    "\n",
    "    print('Write configuration to:', os.path.normpath(fn))\n",
    "\n",
    "else:\n",
    "    fn = os.path.join(log_dir, 'trainer_config.yaml')\n",
    "    with open(fn, 'r') as f:\n",
    "        params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    print('Load configuration from:', os.path.normpath(fn))\n",
    "\n",
    "    assert params['model'].get('__version__') == models.__version__, 'Model version mismatch'\n",
    "    assert params['fit_generator'].get('__version__') == generators.__version__, 'Generator version mismatch'\n",
    "\n",
    "    params['fit_generator']['initial_epoch'] = INITIAL_EPOCH\n",
    "\n",
    "params = SimpleNamespace(**params)\n",
    "# print('')\n",
    "# print(yaml.dump(params.__dict__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE model consists of four components: encoder, latent sampling, decoder, and a second decoder for prediction. Separate model instances are created for each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes a sample `x` and returns `z_mean` and `z_log_var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Encoder(**params.model, name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent sampling takes the two inputs `z_mean` and `z_log_var` and returns a set of `set_size=1` random latent sample `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sampling = models.LatentSampling(**params.model, name='latent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder, finally, takes a latent sample `z` and returns the decoded output `y` to reconstruct `x`. The decoding works backward in time and we set `output_reverse=True` so that the order of decoder output matches the input to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = models.Decoder(output_shape=params.model.get('input_shape'),\n",
    "                         decoder_blocks=params.model.get('encoder_blocks'),\n",
    "                         output_reverse=True,\n",
    "                         **params.model,\n",
    "                         name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the decoder, the second decoder takes the same latent sample `z` and it's output will provide the prediction. In contrast to the `decoder`, we set `output_reverse=False` so that the output of `prediction` is forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = models.Decoder(output_shape=params.model.get('prediction_shape'),\n",
    "                            output_reverse=False,\n",
    "                            **{\n",
    "                                'decoder_blocks': params.model.get('encoder_blocks'),\n",
    "                                **params.model,\n",
    "                                **params.prediction\n",
    "                            },\n",
    "                            name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the four components, we a ready to create the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.VAEp(encoder, decoder, latent_sampling, prediction, **params.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.utils.plot_model(model, show_shapes=True, dpi=75, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and summarizes the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### Load model weights (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we load model weights if `INITIAL_EPOCH > 0` to continue the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if INITIAL_EPOCH > 0:\n",
    "    model_file = os.path.join(log_dir, 'model.{epoch:02d}.h5'.format(epoch=INITIAL_EPOCH))\n",
    "    model.load_weights(model_file, by_name=True)\n",
    "    print('Load model weights from:', os.path.normpath(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we load the data. The data is prepared with the CDO scripts. They are available on Github: https://andr-groth.github.io/cdo-scripts.\n",
    "\n",
    "We use CMIP6 data. The raw gridded data is prepared with the `prepare_data.sh` script. The script performs three main steps: creating anomalies, calculating ensemble EOFs, and obtaining individual PCs. The resulting data is stored in netCDF files and the folder is specified in `params.data['filename']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMIP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the netCDF files of CMIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_variables, _dimensions, _attributes = fileio.read_netcdf_multi(**params.data, num2date=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the netCDF files and their variables by the global attributes `source_id` + `experiment_id`. The attribute `source_id` refers to the model name (e.g. `ACCESS-CM2`) and the attribute `experiment_id` to the experiment (e.g. `ssp126`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {}\n",
    "dimensions = {}\n",
    "attributes = {}\n",
    "\n",
    "key1 = 'source_id'\n",
    "key2 = 'experiment_id'\n",
    "for key, values in _variables.items():\n",
    "    target_key = (\n",
    "        _attributes[key]['.'][key1],\n",
    "        _attributes[key]['.'][key2],\n",
    "    )\n",
    "\n",
    "    variables.setdefault(target_key, {})\n",
    "    dimensions.setdefault(target_key, {})\n",
    "    attributes.setdefault(target_key, {})\n",
    "\n",
    "    variables[target_key] |= {k: pd.DataFrame(v, index=_dimensions[key]['time']) for k, v in values.items()}\n",
    "    dimensions[target_key] |= {k: _dimensions[key] for k in values.keys()}\n",
    "    attributes[target_key] |= {k: _attributes[key] for k in values.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a few tests to check the integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_names = {tuple(val.keys()) for val in variables.values()}\n",
    "if len(variable_names) > 1:\n",
    "    raise ValueError(f'More than one variable combination found: {variable_names}')\n",
    "else:\n",
    "    variable_names, *_ = variable_names\n",
    "    print('\\N{check mark} One variable combination found:', variable_names)\n",
    "\n",
    "variable_channels = {tuple(v.shape[-1] for v in val.values()) for val in variables.values()}\n",
    "if len(variable_channels) > 1:\n",
    "    raise ValueError(f'More than one channel combination found: {variable_channels}')\n",
    "else:\n",
    "    variable_channels, *_ = variable_channels\n",
    "    print('\\N{check mark} One channel combination found:', variable_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the models and their different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of model runs found :', len(variables))\n",
    "df = pd.crosstab(*list(zip(*list(variables.keys()))), rownames=[key1], colnames=[key2])\n",
    "df.loc['--- Total ---'] = df.sum(axis=0)\n",
    "display(df.replace(0, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, we compare the different CMIP datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 3\n",
    "rows = 2\n",
    "\n",
    "dataset_names = list(variables.keys())\n",
    "_, first_idx, color_idx = np.unique([dataset_name[1] for dataset_name in dataset_names],\n",
    "                                    return_index=True,\n",
    "                                    return_inverse=True)\n",
    "\n",
    "cm = plt.cm.get_cmap('tab10', 10)\n",
    "color_dict = {name: cm(idx) for name, idx in zip(dataset_names, color_idx)}\n",
    "\n",
    "for variable_name in variable_names:\n",
    "    fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(FIGWIDTH, 3 * rows), squeeze=False)\n",
    "    fig.suptitle(variable_name.upper(), fontweight='bold')\n",
    "    for idx, dataset_name in enumerate(dataset_names):\n",
    "        for channel, (ax, values) in enumerate(zip(axs.flat, variables[dataset_name][variable_name].values.T)):\n",
    "            label = dataset_name[1] if idx in first_idx else None\n",
    "            ax.plot(variables[dataset_name][variable_name].index,\n",
    "                    values,\n",
    "                    color=color_dict[dataset_name],\n",
    "                    label=label,\n",
    "                    alpha=0.5)\n",
    "\n",
    "            ax.set_title(f'Channel {channel}')\n",
    "\n",
    "    axs.flat[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the variance of the different CMIP runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = len(variable_names)\n",
    "fig, axs = plt.subplots(2, cols, figsize=(FIGWIDTH, 4 * cols), sharex=True, squeeze=False, gridspec_kw={'wspace': 0.3})\n",
    "fig.subplots_adjust()\n",
    "\n",
    "for (uax, bax), variable_name in zip(axs.T, variable_names):\n",
    "    uax.set_title(variable_name.upper(), fontweight='bold')\n",
    "\n",
    "    for idx, dataset_name in enumerate(dataset_names):\n",
    "        data_var = variables[dataset_name][variable_name].var(axis=0)\n",
    "        label = dataset_name[1] if idx in first_idx else None\n",
    "        uax.plot(data_var, 'o', color=color_dict[dataset_name], label=label, alpha=0.5)\n",
    "\n",
    "        data_cvar = np.cumsum(data_var, axis=0)\n",
    "        bax.plot(data_cvar, '-o', color=color_dict[dataset_name], alpha=0.5)\n",
    "\n",
    "    uax.set_ylabel('Variance')\n",
    "    uax.grid(linestyle=':')\n",
    "\n",
    "    bax.set_xlabel('Channel')\n",
    "    bax.set_ylabel('Cumulative variance')\n",
    "    bax.grid(linestyle=':')\n",
    "    bax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
    "\n",
    "    axs.flat[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training, we shuffle the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names_rand = list(variables.keys())\n",
    "random.seed(params.fit_generator['random_seed'])\n",
    "random.shuffle(dataset_names_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables are stack along the last axis, the channels. We add a leading singleton dimension for `set_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_stack = [\n",
    "    pd.concat([variables[dataset_name][variable_name] for variable_name in variable_names], axis=1, join='inner')\n",
    "    for dataset_name in dataset_names_rand\n",
    "]\n",
    "time = [d.index for d in data_stack]\n",
    "dataset = [d.to_numpy()[None, ...] for d in data_stack]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the datasets into one set for training and one set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = params.fit_generator['validation_split']\n",
    "print('Size of training dataset   :', len(dataset[:validation_split]))\n",
    "print('Size of validation dataset :', len(dataset[validation_split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forcing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the forcing data from a csv file. The first column is considered as the date and the remaining columns is the corresponding forcing data. The column header of the forcing data must match the `experiment_id`. The forcing data is interpolated to match the time scale of the CMIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = params.forcing['filename']\n",
    "forcing_df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "print('Load forcing data from:', os.path.relpath(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_df_daily = forcing_df.asfreq('D').interpolate(method='akima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "for n, (key, val) in enumerate(forcing_df.items()):\n",
    "    ax.plot(val, 'o', color=cm(n), label=key)\n",
    "    ax.plot(forcing_df_daily[key], '-', color=cm(n))\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of providing only the current value of the forcing as condition, we can optionally provide past values as well. The parameter `input_length` in `params.forcing` determines the number of past values (in years) that will be provided as condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = params.forcing.get('scale', 1)\n",
    "input_length = params.forcing.get('input_length', 1)\n",
    "\n",
    "forcing_data = [\n",
    "    np.stack([\n",
    "        forcing_df_daily[experiment_id].reindex(t - pd.DateOffset(years=years), method='nearest').to_numpy() * scale\n",
    "        for years in range(input_length)\n",
    "    ],\n",
    "             axis=1) for t, (_, experiment_id) in zip(time, dataset_names_rand)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model training, we first need a scheduler for the beta values in each epoch that scales the KL loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BetaScheduler = getattr(beta_schedulers, params.beta_scheduler.get('class_name'))\n",
    "beta_scheduler = BetaScheduler(**params.beta_scheduler.get('config'))\n",
    "beta_scheduler.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first generator takes the first set of the CMIP data. This generator is used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = [t.to_numpy().astype('datetime64[M]').astype(int) for t in time]\n",
    "_, model_index = np.unique([dataset_name[0] for dataset_name in dataset_names_rand], return_inverse=True)\n",
    "fit_gen = generators.FitGenerator(dataset[:validation_split],\n",
    "                                  **params.fit_generator,\n",
    "                                  beta_scheduler=beta_scheduler,\n",
    "                                  time=month[:validation_split],\n",
    "                                  ensemble_index=model_index[:validation_split],\n",
    "                                  condition=forcing_data[:validation_split])\n",
    "fit_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second generator takes the second set of CMIP data. The generator is used to evaluate the sucess of the training to generalize to independent CMIP runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset[validation_split:]:\n",
    "    val_gen = generators.FitGenerator(dataset[validation_split:],\n",
    "                                      **params.fit_generator,\n",
    "                                      beta_scheduler=beta_scheduler,\n",
    "                                      time=month[validation_split:],\n",
    "                                      ensemble_index=model_index[validation_split:],\n",
    "                                      condition=forcing_data[validation_split:])\n",
    "    val_gen.summary()\n",
    "else:\n",
    "    val_gen = None\n",
    "    print('No validation data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callbacks are used to save the model weights and evaluation metrics in the `LOG_DIR`. The progress of the training can be monitored with Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=os.path.join(log_dir, 'model.{epoch:02d}.h5'),\n",
    "                    period=5,\n",
    "                    save_best_only=False,\n",
    "                    save_weights_only=True),\n",
    "    ks.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=False),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model.fit(fit_gen,\n",
    "                 validation_data=val_gen,\n",
    "                 initial_epoch=INITIAL_EPOCH,\n",
    "                 **params.model_fit,\n",
    "                 callbacks=callbacks + [TqdmCallback(verbose=0)],\n",
    "                 verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
